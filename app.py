import streamlit as st
import google.generativeai as genai
from pinecone import Pinecone, ServerlessSpec
import os
import json
import dotenv

# --- Streamlit UIè¨­å®š ---
st.set_page_config(page_title="Pineconeé€£æºQAãƒœãƒƒãƒˆ", layout="wide", initial_sidebar_state="collapsed")
st.markdown("""
    <style>
    #MainMenu, header, footer {visibility: hidden;}
    </style>
""", unsafe_allow_html=True)

# --- ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ (.envã¾ãŸã¯secrets.toml) ---
dotenv.load_dotenv()
API_KEY = os.getenv("API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = "pdf-qa-bot"
PINECONE_REGION = "us-west-2"  # ä½¿ç”¨ã—ã¦ã„ã‚‹ç’°å¢ƒã«å¿œã˜ã¦å¤‰æ›´å¯ï¼ˆä¾‹: "gcp-starter"ï¼‰

# --- Gemini åˆæœŸåŒ– ---
genai.configure(api_key=API_KEY)
embed_model = genai.GenerativeModel("embedding-001")
chat_model = genai.GenerativeModel("gemini-1.5-pro")

# --- Pinecone åˆæœŸåŒ– ---
pc = Pinecone(api_key=PINECONE_API_KEY)

# --- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ï¼‰ ---
if PINECONE_INDEX_NAME not in pc.list_indexes().names():
    with st.spinner("ğŸ”§ Pineconeã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆä¸­..."):
        pc.create_index(
            name=PINECONE_INDEX_NAME,
            dimension=768,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region=PINECONE_REGION)  # é©å®œå¤‰æ›´
        )
        st.success(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ `{PINECONE_INDEX_NAME}` ã‚’ä½œæˆã—ã¾ã—ãŸ")

# --- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸æ¥ç¶š ---
index = pc.Index(PINECONE_INDEX_NAME)

# --- Streamlit è³ªå•ãƒ•ã‚©ãƒ¼ãƒ  ---
with st.form("qa_form"):
    question = st.text_input("â“ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", value=st.session_state.get("question", ""))
    submitted = st.form_submit_button("è³ªå•ã™ã‚‹")

# --- å›ç­”å‡¦ç† ---
if submitted and question:
    st.session_state["question"] = question
    with st.spinner("ğŸ” å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."):

        try:
            # --- è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ– ---
            user_embedding = embed_model.embed_content(question, task_type="retrieval_query")["embedding"]

            # --- Pineconeæ¤œç´¢ ---
            results = index.query(vector=user_embedding, top_k=5, include_metadata=True)

            # --- çµæœã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã¾ã¨ã‚ã‚‹ ---
            context = ""
            for match in results["matches"]:
                meta = match["metadata"]
                source = meta.get("source", "ä¸æ˜ãƒ•ã‚¡ã‚¤ãƒ«")
                chunk = match.get("values") or ""
                context += f"\n\n--- {source} ---\n{chunk}"

            # --- Geminiã«è³ªå• ---
            prompt = f"""ä»¥ä¸‹ã®ç¤¾å†…æ–‡æ›¸ã‚’å‚è€ƒã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

{context}

Q: {question}
"""
            response = chat_model.generate_content(prompt)
            answer = response.text if hasattr(response, "text") else response.candidates[0]['content']['parts'][0]['text']

            st.session_state["answer"] = answer

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# --- å›ç­”è¡¨ç¤º ---
if st.session_state.get("answer"):
    st.markdown("### âœ… å›ç­”ï¼š")
    st.write(st.session_state["answer"])

    if st.button("ã‚¯ãƒªã‚¢"):
        for key in ["question", "answer"]:
            st.session_state.pop(key, None)
        st.rerun()
