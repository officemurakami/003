import streamlit as st
import google.generativeai as genai
from pinecone import Pinecone, ServerlessSpec
import os
import dotenv

# --- åˆæœŸè¨­å®š ---
st.set_page_config(page_title="Pineconeé€£æºQAãƒœãƒƒãƒˆ", layout="wide", initial_sidebar_state="collapsed")
st.markdown("""
    <style>
    #MainMenu, header, footer {visibility: hidden;}
    </style>
""", unsafe_allow_html=True)

# --- ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ ---
dotenv.load_dotenv()
API_KEY = os.getenv("API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = "pdf-qa-bot"
PINECONE_REGION = "us-east-1"
PINECONE_CLOUD = "aws"

# --- Gemini åˆæœŸåŒ– ---
genai.configure(api_key=API_KEY)
embed_model = genai.GenerativeModel("embedding-001")
chat_model = genai.GenerativeModel("gemini-1.5-pro")

# --- Pinecone åˆæœŸåŒ– ---
pc = Pinecone(api_key=PINECONE_API_KEY)

# --- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ï¼‰
st.markdown("### ğŸ“¦ Pineconeã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§")
try:
    index_list = pc.list_indexes().names()
    st.write(index_list)
except Exception as e:
    st.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    index_list = []

# --- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ï¼‰
if PINECONE_INDEX_NAME not in index_list:
    with st.spinner("ğŸ”§ Pineconeã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆä¸­..."):
        try:
            pc.create_index(
                name=PINECONE_INDEX_NAME,
                dimension=1024,  # ã‚ãªãŸã®ç’°å¢ƒã®è¨­å®šã«åˆã‚ã›ã¦å¤‰æ›´æ¸ˆã¿
                metric="cosine",
                spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION)
            )
            st.success(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ `{PINECONE_INDEX_NAME}` ã‚’ä½œæˆã—ã¾ã—ãŸ")
        except Exception as e:
            st.error(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

# --- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¸æ¥ç¶š
try:
    index = pc.Index(PINECONE_INDEX_NAME)
except Exception as e:
    st.error(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")

# --- Streamlit è³ªå•ãƒ•ã‚©ãƒ¼ãƒ 
with st.form("qa_form"):
    question = st.text_input("â“ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", value=st.session_state.get("question", ""))
    submitted = st.form_submit_button("è³ªå•ã™ã‚‹")

# --- å›ç­”å‡¦ç†
if submitted and question:
    st.session_state["question"] = question
    with st.spinner("ğŸ” å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™..."):
        try:
            user_embedding = embed_model.embed_content(
                question,
                task_type="retrieval_query"
            )["embedding"]

            results = index.query(vector=user_embedding, top_k=5, include_metadata=True)

            context = ""
            for match in results["matches"]:
                meta = match["metadata"]
                source = meta.get("source", "ä¸æ˜ãƒ•ã‚¡ã‚¤ãƒ«")
                chunk = match.get("values") or ""
                context += f"\n\n--- {source} ---\n{chunk}"

            prompt = f"""ä»¥ä¸‹ã®ç¤¾å†…æ–‡æ›¸ã‚’å‚è€ƒã«è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

{context}

Q: {question}
"""
            response = chat_model.generate_content(prompt)
            answer = response.text if hasattr(response, "text") else response.candidates[0]['content']['parts'][0]['text']
            st.session_state["answer"] = answer

        except Exception as e:
            st.error(f"âŒ å›ç­”ç”Ÿæˆä¸­ã®ã‚¨ãƒ©ãƒ¼: {e}")

# --- å›ç­”è¡¨ç¤º
if st.session_state.get("answer"):
    st.markdown("### âœ… å›ç­”ï¼š")
    st.write(st.session_state["answer"])

    if st.button("ã‚¯ãƒªã‚¢"):
        for key in ["question", "answer"]:
            st.session_state.pop(key, None)
        st.rerun()
